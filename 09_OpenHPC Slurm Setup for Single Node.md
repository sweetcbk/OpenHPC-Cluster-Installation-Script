## Pre-setup
\
**Master and Compute**\
Hostname: sirius\
enp0s8: public network 10.204.100.10 (depend on your ip address)

<br>

### Add host
```
# vi /etc/hosts

10.204.100.10 sirius
```

```
hostnamectl set-hostname sirius
```

### Disable firewall
```
# systemctl disable firewalld
# systemctl stop firewalld
```

### Disable selinux
```
# vi /etc/selinux/config

SELINUX=disabled
```

### Reboot node 
```
# reboot
```

### Update CentOS
```
# yum -y update
```
<br>

## OpenHPC and Slurm Setup

<br>

### Install OpenHPC Repository
```
# yum -y install http://repos.openhpc.community/OpenHPC/2/CentOS_8/x86_64/ohpc-release-2-1.el8.x86_64.rpm
```

### Install basic package for OpenHPC
```
# yum -y install ohpc-base
# yum -y install ohpc-warewulf
```

### Install Slurm
```
# yum -y install ohpc-slurm-server
# yum -y install ohpc-slurm-client
# cp /etc/slurm/slurm.conf.ohpc /etc/slurm/slurm.conf
```

### Restart and enable services
```
# systemctl enable mariadb.service 
# systemctl restart mariadb
# systemctl enable httpd.service
# systemctl restart httpd
# systemctl enable dhcpd.service
```

### Install OpenHPC for compute node
```
# yum -y install ohpc-base-compute
```

### Install modules user enviroment for compute node and master node
```
# yum -y install lmod-ohpc
```

### Create basic values for OpenHPC
```
# wwinit database 
# wwinit ssh_keys
```

### Update basic slurm configuration by vim
Please adjust configuration based on your computer. BTW, you can adjust by using configuration tool at https://slurm.schedmd.com/configurator.html
```
# vi /etc/slurm/slurm.conf

#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=sirius.cluster
ControlMachine=sirius
#ControlAddr=
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
#PluginDir=
#FirstJobId=
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
SelectType=select/cons_res
SelectTypeParameters=CR_CPU_Memory
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=
#
# ACCOUNTING
#JobAcctGatherType=jobacct_gather/linux
#JobAcctGatherFrequency=30
#
#AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageHost=
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStorageUser=
#
# COMPUTE NODES
# OpenHPC default configuration
PropagateResourceLimitsExcept=MEMLOCK
#AccountingStorageType=accounting_storage/filetxt
#Epilog=/etc/slurm/slurm.epilog.clean
GresTypes=gpu
#
# NODES
NodeName=sirius NodeAddr=sirius NodeHostName=sirius Gres=gpu:2 CPUs=12 CoresPerSocket=12 ThreadsPerCore=1 RealMemory=31000 St
ate=UNKNOWN
#
# PARTITIONS
PartitionName=gpu Nodes=sirius DefMemPerCPU=512 Default=YES Shared=NO State=UP MaxTime=INFINITE
PartitionName=cpu Nodes=sirius DefMemPerCPU=512 Default=YES Shared=NO State=UP MaxTime=INFINITE
#
ReturnToService=0

```

### Add gres.conf for GPU allocation
Please check number of GPU, e.g. nvidia[0-1] is 2 GPUs
```
# vi /etc/slurm/gres.conf

NodeName=sirius Name=gpu File=/dev/nvidia[0-1]
```

### Restart Slurm and Munge services
```
# systemctl enable munge
# systemctl enable slurmctld
# systemctl enable slurmd
# systemctl start munge
# systemctl start slurmctld
# systemctl start slurmd
```

### Determine memlock values
```
# perl -pi -e 's/# End of file/\* soft memlock unlimited\n$&/s' /etc/security/limits.conf
# perl -pi -e 's/# End of file/\* hard memlock unlimited\n$&/s' /etc/security/limits.conf
# perl -pi -e 's/# End of file/\* soft memlock unlimited\n$&/s' $CHROOT/etc/security/limits.conf 
# perl -pi -e 's/# End of file/\* hard memlock unlimited\n$&/s' $CHROOT/etc/security/limits.conf
```

### Test resource by
```
# scontrol show nodes
```

### Installation essential modules/softwares
<br>

#### Install Intel Complier (Intel oneAPI Base)
Ref: https://software.intel.com/content/www/us/en/develop/tools/oneapi/base-toolkit/download.html?operatingsystem=linux&distributions=yumpackagemanager
```
sudo bash -c 'cat << EOF > /etc/yum.repos.d/oneAPI.repo
[oneAPI]
name=Intel(R) oneAPI repository
baseurl=https://yum.repos.intel.com/oneapi
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://yum.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB
EOF'
```
```
sudo yum upgrade intel-basekit
```
<br>

#### Install OHPC basic additional packages
You can check additional packages from http://repos.openhpc.community/OpenHPC/2/CentOS_8/x86_64/
```
# yum install -y EasyBuild-ohpc autoconf-ohpc automake-ohpc cmake-ohpc gnu9-compilers-ohpc hwloc-ohpc libtool-ohpc python3-Cython-ohpc singularity-ohpc
```
<br>

#### Install OHPC GNU9 basic additional packages
You can check additional packages from http://repos.openhpc.community/OpenHPC/2/CentOS_8/x86_64/
```
# yum install -y ohpc-gnu9* R-gnu9-ohpc adios-gnu9* boost-gnu9* fftw-gnu9* hdf5-gnu9* mpich-ofi-gnu9* mpich-ucx-gnu9* mvapich2-gnu9* netcdf-gnu9* openmpi4-gnu9* pdtoolkit-gnu9* phdf5-gnu9* pnetcdf-gnu9* python3-mpi4py-gnu9* sionlib-gnu9* 
```
<br>

#### Install OHPC Intel basic additional packages
You can check additional packages from http://repos.openhpc.community/OpenHPC/2/CentOS_8/x86_64/
```
# yum install -y ohpc-intel* intel-compilers-devel-ohpc intel-mpi-devel-ohpc adios-intel* boost-intel* hdf5-intel* mpich-ofi-intel* mpich-ucx-intel* mvapich2-intel* netcdf-intel* openmpi4-intel* pdtoolkit-intel* phdf5-intel* pnetcdf-intel* python3-mpi4py-intel* sionlib-intel* 
```
